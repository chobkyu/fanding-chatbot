from langgraph.graph import StateGraph
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_community.chat_models import ChatOpenAI
from typing import TypedDict, List, Union
from langchain.prompts import ChatPromptTemplate
from langchain.agents import create_openai_functions_agent
from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser
from langchain.schema.agent import AgentFinish  # ðŸ”§ ì¶”ê°€

# ðŸ”§ ë„êµ¬ ìž„í¬íŠ¸
from tools import search_tool
from main import retriver_tool
tools = [retriver_tool, search_tool]

# ðŸ¤– LLM ë° Agent ì„¸íŒ…
llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0.7)
prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ íŒ¬ë”© ê³ ê°ì„¼í„° AI ì±—ë´‡ìž…ë‹ˆë‹¤. ê³ ê°ì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”."),
    ("user", "{input}"),
    ("ai", "{agent_scratchpad}")
])
agent = create_openai_functions_agent(
    llm=llm,
    tools=tools,
    prompt=prompt
)
parser = OpenAIFunctionsAgentOutputParser()

# ðŸ“¦ ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    input: str
    chat_history: List[Union[HumanMessage, AIMessage, ToolMessage]]
    output: str

# ðŸ§  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
system_message = SystemMessage(content="ë‹¹ì‹ ì€ íŒ¬ë”© ê³ ê°ì„¼í„° AI ì±—ë´‡ìž…ë‹ˆë‹¤. ê³ ê°ì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”.")

# ðŸ”§ intermediate_steps ì¶”ì¶œ í•¨ìˆ˜
def extract_intermediate_steps(messages: List[Union[HumanMessage, AIMessage, ToolMessage]]):
    steps = []
    for i in range(len(messages) - 1):
        if isinstance(messages[i], AIMessage) and isinstance(messages[i + 1], ToolMessage):
            steps.append((messages[i], messages[i + 1]))
    return steps

# ðŸ§  LangGraph ì‹¤í–‰ í•¨ìˆ˜
def run_with_memory_and_tools(state: AgentState) -> AgentState:
    user_input = HumanMessage(content=state["input"])
    history = state["chat_history"]
    intermediate_steps = extract_intermediate_steps(history)

    agent_output = agent.invoke({
        "input": state["input"],
        "intermediate_steps": intermediate_steps
    })

    print(agent_output)
    new_history = history + [user_input]

    # ðŸ”§ AgentFinish ì‘ë‹µ ì²˜ë¦¬
    if isinstance(agent_output, AgentFinish):
        final_msg = AIMessage(content=agent_output.return_values['output'])
        new_history.append(final_msg)
        return {
            "input": "",
            "chat_history": new_history,
            "output": final_msg.content
        }

    # ðŸ”§ tool calling ì²˜ë¦¬
    if isinstance(agent_output, AIMessage) and "tool_calls" in agent_output.additional_kwargs:
        tool_messages = []
        for tool_call in agent_output.additional_kwargs["tool_calls"]:
            func_name = tool_call["function"]["name"]
            arguments = eval(tool_call["function"]["arguments"])
            for t in tools:
                if t.name == func_name:
                    result = t.func(**arguments)
                    tool_messages.append(ToolMessage(
                        tool_call_id=tool_call["id"],
                        content=str(result)
                    ))

        # ðŸ”§ ìµœì¢… LLM ì‘ë‹µ
        final_messages = [system_message] + new_history + [agent_output] + tool_messages
        final_response = llm.invoke(final_messages)

        new_history += [agent_output] + tool_messages + [final_response]

        return {
            "input": "",
            "chat_history": new_history,
            "output": final_response.content
        }

    # ðŸ”§ fallback: ê·¸ëƒ¥ AIMessage
    if isinstance(agent_output, AIMessage):
        new_history.append(agent_output)
        return {
            "input": "",
            "chat_history": new_history,
            "output": agent_output.content
        }

    # ðŸ”§ ì˜ˆì™¸ ì²˜ë¦¬
    return {
        "input": "",
        "chat_history": new_history,
        "output": "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    }

# ðŸ§± ê·¸ëž˜í”„ êµ¬ì„±
graph = StateGraph(AgentState)
graph.add_node("chatbot", RunnableLambda(run_with_memory_and_tools))
graph.set_entry_point("chatbot")
graph.set_finish_point("chatbot")
app = graph.compile()

# ðŸªœ ì „ì—­ ížˆìŠ¤í† ë¦¬
chat_history: List[Union[HumanMessage, AIMessage, ToolMessage]] = []

# ðŸŽ¯ í˜¸ì¶œ í•¨ìˆ˜
def get_agent_answer_graph_history(query: str) -> str:
    global chat_history

    state = {
        "input": query,
        "chat_history": chat_history,
        "output": ""
    }

    result = app.invoke(state)
    print(result)
    chat_history = result["chat_history"]
    return result["output"]
